# Loan-approval-prediction
## Dataset
Source: Kaggle — bank_loan_data by Uday Malviya
https://www.kaggle.com/datasets/udaymalviya/bank-loan-data
## Exploratory Data Analysis (EDA)
 • Dataset Overview: The dataset contains 45,000 loan applicant records with 14 columns about personal demographics (age, gender, education), financial details (income, employment experience, home ownership), loan attributes (amount, intent/purpose, interest rate, loan-to-income ratio), credit history length, credit score, an indicator of any previous loan defaults, and the loan approval status. The target variable loan_status is binary (0 = rejected, 1 = approved). In this dataset, 77.8% of applications were not approved and 22.2% were approved, indicating a class imbalance. No missing values were found in any column.
 • Distribution of Features: person_income is right-skewed (median ~67k, mean ~80k) with some high-income outliers (max ~7.2 million). person_age is mostly in the 20-40 range (median ~26) but with a few improbable outliers (max 144 years). person_emp_exp (work experience) similarly had outliers (up to 125 years). Other features showed reasonable ranges (loan_amnt from $500 to $35,000, loan_int_rate from ~5% to 20%). Categorical variables like person_home_ownership were mostly "RENT" or "MORTGAGE", and loan_intent was distributed among purposes like Education, Medical, Venture, etc.
 • Correlation Analysis: some features are highly correlated with each other. For instance, person_age and person_emp_exp (years of experience) have a correlation 0.95, as older applicants tend to have more work experience. Similarly, person_age is strongly correlated (0.86) with cb_person_cred_hist_length (credit history years). Most features had low correlation with the target loan_status, except a few: loan_percent_income (ratio of loan amount to income) and loan_int_rate had the highest positive correlations (0.38 and 0.33 respectively) with loan approval. This indicates that approved loans tended to involve higher interest rates and higher loan-to-income ratios.
 • Feature Distributions & Normality: most continuous features were not normally distributed (income was skewed, credit scores roughly bell-shaped but slightly skewed). A Shapiro-Wilk test on person_income confirmed non-normality (p < 0.001). Therefore, techniques assuming normality would not be applicable without transformations; however, tree-based models can handle skewed distributions and scaling can be applied to linear models. I also visualized categorical features: for instance, a bar plot of loan_intent showed Education loans were most common, followed by Medical and Venture loans.
## Data Preprocessing
 • Encoding Categorical Features: binary categories person_gender and previous_loan_default_on_file were label-encoded (male=1/female=0, Yes=1/No=0). Other categorical features were one-hot encoded with dummy variables. This resulted in a feature set of 22 dimensions after encoding (from the original 13 features).
 • Feature Scaling: feature scaling to the numeric features to aid models like Logistic Regression and SVM. StandardScaler standardized continuous variables (age, income, experience, loan amount, interest rate, loan_percent_income, credit history length, credit score) to have mean 0 and unit variance. This ensures that variables on different scales (income in tens of thousands vs. interest rates in single digits) are normalized, preventing any feature from dominating the distance-based algorithms. Note that tree-based models are scale-invariant, but scaling does not harm them and is essential for SVM/logistic.
 • Train-Test Split: split the data into training and test sets before model training. I used an 80/20 stratified split (train = 36,000 samples, test = 9,000) to preserve the approval rate proportion in each set. The stratification ensures the minority class (approved loans) is represented in both train and test splits. The test set will be used for final model performance evaluation, while all model training and hyperparameter tuning is performed via cross-validation on the training set.
## Model Training and Hyperparameter Tuning
For each model, I performed hyperparameter tuning using GridSearchCV (with 5-fold cross-validation on the training set) to find the optimal parameters. I used the ROC AUC score during tuning to account for class imbalance. Below are details for each model:
1. Logistic Regression
Logistic regression is fast, interpretable, and outputs well-calibrated probabilities. I tuned the regularization strength C (inverse of regularization parameter) using grid search: the best C was 10 (weaker regularization), indicating that a more flexible model fit the data better. The cross-validated AUC for the best logistic model was 0.953.
3. Decision Tree
Decision trees can capture non-linear relationships and interactions. To prevent overfitting, I tuned the tree depth and minimum samples per split: the optimal tree depth was 8 levels. A deeper tree (or no depth limit) tended to overfit. The tuned decision tree obtained a slightly higher AUC (0.965) than logistic/SVM on the validation folds.
4. Random Forest
Random Forests combine many decision trees (with bootstrap sampling and random feature selection for each split) to improve generalization. Tuned the number of trees and tree depth: the best model used 300 trees and no depth limit (the forest can fully grow trees since averaging reduces overfit). The Random Forest was a top performer, with cross-val AUC 0.975.
5. XGBoost
I used the XGBClassifier from the XGBoost library. Tuned parameters such as the learning rate, max tree depth, and number of estimators: best XGBoost model had ROC AUC 0.979, which is a bit better that RF.
6. CatBoost
Finally, trained a CatBoost classifier, another gradient boosting algorithm particularly good with categorical features. CatBoost can natively handle categorical variables by internal encoding, though in our case they were already one-hot encoded. I performed a similar parameter search for CatBoost (iterations/trees, depth, learning rate). The CatBoost model achieved performance in line with XGBoost and Random Forest.
## Model Evaluation
After tuning, I evaluated each final model on the test set (9000 samples) using multiple metrics: Accuracy, Precision, Recall, F1-score, ROC AUC, and PR AUC (Area under Precision-Recall curve). These metrics provide a full view:
• Accuracy: overall, what part of loans were correctly classified.
 • Precision: of those predicted as approved, how many were actually approved.
 • Recall: of the actual approved loans, how many did the model identify.
 • F1-score: harmonic mean of precision and recall, balancing both.
 • ROC AUC: measures the model's ability to distinguish classes across limits; 1.0 is perfect, 0.5 is chance.
 • PR AUC: focuses on performance on the positive class; useful for imbalanced data as it highlights precision-recall trade-off.
## Test Set Results:
As shown above, the ensemble models (Random Forest, XGBoost, CatBoost) performed best on the test data, with CatBoost slightly edging out the others in metrics. All three achieved high accuracy (~92-94%) and high ROC AUC (~0.97-0.98). The Precision-Recall AUC was also highest for these models, indicating they handle the class imbalance well by achieving high precision and recall simultaneously. Logistic Regression and SVM trailed slightly with ~85% accuracy and lower recall, likely because the decision border is linear and less flexible. The single Decision Tree had better accuracy (~93%) and ROC AUC, as it handled nonlinear behavior better, but it lacked the averaging benefit of ensemble methods. I also plotted the ROC and Precision-Recall curves for each model.
## Feature Importance
For the tree-based models, I extracted feature importances to understand which factors most strongly influence loan approval:
 • Previous Loan Defaults: This was the top feature by far in all tree-based models. This aligns with intuition that a history of default greatly reduces approval chances.
 • Loan Percent Income: ratio of loan amount to income was the second most important feature. Higher loan-to-income ratios were associated with approvals, probably because the bank considers the requested amount reasonable relative to income.
 • Interest Rate: higher interest rates correlated with approvals (perhaps reflecting risk-based pricing – riskier loans come with higher rates).
 • Applicant Income: the model found that lower income (coupled with other factors like smaller loans or higher relative interest) could still lead to approval, which might imply that high-income applicants either did not seek loans as frequently or were not always approved if the loan wasn't profitable enough.
 • Other features like Loan Amount, Home Ownership status, and Credit Score had moderate importance. For example, renters were slightly less likely to be approved than mortgage holders, according to the model.
 • Education level and Loan intent were among the least influential features in the models. This suggests the approval decision was driven more by financial factors and credit history than by an applicant’s education or the stated purpose of the loan.
 ## Conclusion
 In summary, previous loan defaults, loan-to-income ratio, and interest rate turned out to be the most important features influencing loan approval decisions. The CatBoost model was the top performer in predicting loan approvals. The chosen model is not only accurate but also understood its driving factors, which is essential for practical deployment in loan risk assessment.